Óscar Vilela Rodríguez y Ainhoa de Diego Silva 
oscar.vilela.rodriguez@udc.es; ainhoa.dediego.silva@udc.es

INTRODUCCIÓN

En esta segunda práctica analizamos y demostramos de manera empírica la fiabilidad, tiempo de ejecución, y complejidad de dos algoritmos de ordenación propuestos que realizan la ordenación ascedente de un vector, que se inizializa de tres formas posibles; ascedente, descendente y aleatorio. Para ello, creamos un programa en Python, compuesto de varios archivos de código: 

Algoritmos.py 
FuncionesTests.py 
FuncionesAuxiliares.py 
Main.py

ACLARACIONES INICIALES

Para ejecutar el código debes moverte al directorio y escribir en la terminal python main.py.

Las características de la máquina que usamos se muestran a continuación: 

Nombre del modelo: Lenovo Legion 5 15ITH6H Intel Core i7-11800H/32GB/1TB SSD/RTX3070/15.6" 
Procesador Intel Core i7-11800H (8C / 16T, 2.3 / 4.6GHz, 24MB) 
Memoria RAM 2x 16GB SO-DIMM DDR4-3200 
Almacenamiento 1TB SSD M.2 2280 PCIe 3.0x4 NVMe 
Python 3.9.12

*Este programa en específico se ejecutó en el procesador (CPU) por lo que la tarjeta gráfica (GPU) no es necesaria para su ejecución. 
*Todas las medidas de tiempo han sido tomadas en nanosegundos (ns), mediante la función 	time.perf_counter_ns() , y mostradas por pantalla a su vez en nanosegundos. 
*Todas las funciones aquí comentadas están descritas en mayor profundidad en el código. 


Una vez aclaradas las cuestiones básicas, explicaremos paso a paso, el proceso que nos llevó a las conclusiones finales de este informe.

DESARRROLLO DE LAS PRUEBAS

1º Creamos el archivo Algoritmos.py en el que definimos las funciones de los algoritmos a comprobar.

ins_sort (v : list) --> list

    La función ordena el vector dado según el algoritmo de inserción. Comienza cogiendo como variables x y j, el contenido de i y la posición anterior a i respectivamente. Seguidamente, compara las dos de tal forma que ordena el vector dejando los elementos mayores que x a la derecha.

shell_short_hibbard (v:list) --> list

    La función ordena el vector dado según el algoritmo de ordenación Shell. Utilizando las funciones auxiliares shell_sort_aux(v:list, increments) --> list y hibbard_increments(n) --> increments:list para calcular previamente los incrementos de Hibbard y utilizarlos en la inserción. La función compara el valor del elemento contenido en i con el valor del elemento guardado en la posición j-incremento. Si el primer elemento a comparar es menor que el segundo , los intercambia de posición y reduce el tamaño del incremento. Si no se cumple la condición anterior, sigue con las iteraciones del bucle. 

2º Comprobamos el correcto funcionamiento de los dos algoritmos y la complejidad algorítmica de forma empírica:

Para ello creamos el archivo FuncionesTests.py, que a su vez utiliza otras funciones definidas en FuncionesAuxiliares.py, donde definimos las funciones “test”: 

probar_algoritmos(n:int):
    Nos permite resolver el problema que se plantea con cada algoritmo.


Test 1 (Casos de Prueba):
Inicialización ascendente
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Ordenado inicialmente? True
Ordenado con el algoritmo de inserción? True [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Ordenado con el algoritmo de Shell? True [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

Inicialización descendente
[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]
Ordenado inicialmente? False
Ordenado con el algoritmo de inserción? True [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Ordenado con el algoritmo de Shell? True [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

Inicialización aleatoria
[10, -7, 10, 7, 3, 9, 1, 1, -8, 1]
Ordenado inicialmente? False
Ordenado con el algoritmo de inserción? True [-8, -7, 1, 1, 1, 3, 7, 9, 10, 10]
Ordenado con el algoritmo de Shell? True [-8, -7, 1, 1, 1, 3, 7, 9, 10, 10]


3º Para comprobar el tiempo de ejecución, creamos la función test_tiempo_complejidad(algoritmo, func_type, exp1, exp2, exp3) que calcula el tiempo y nos permite medir la complejidad de manera empírica gracias a las cotas.
Utilizamos un vector de progresión geométrica 2 porque nos ayuda a visualizar hacia qué valor de tiempo tiende el algoritmo.
Los asteriscos en las tablas de datos simbolizan que esos valores están por debajo del umbral de confianza, por lo que utilizamos el método del promedio para tener una medida más precisa.

Test 2 (Análisis de Complejidad): (Iteración nº 5)
***ins_sort*** asc
                                                     Subestimada    Ajustada  Sobreestimada
           n                  t(n) (ns)            t(n)/n^0.8       t(n)/n^1     t(n)/n^1.2
*        128                 11490.0000            236.892905      89.765625      34.014811 (promedio de 10 repeticiones)
*        256                 25780.0000            305.274790     100.703125      33.219643 (promedio de 10 repeticiones)
*        512                 51740.0000            351.892861     101.054688      29.020338 (promedio de 10 repeticiones)
*       1024                100910.0000            394.179687      98.544922      24.636230 (promedio de 10 repeticiones)
*       2048                206190.0000            462.597879     100.678711      21.911477 (promedio de 10 repeticiones)
*       4096                423130.0000            545.237678     103.303223      19.572301 (promedio de 10 repeticiones)
*       8192                885830.0000            655.599222     108.133545      17.835383 (promedio de 10 repeticiones)
       16384               1735500.0000            737.715089     105.926514      15.209702

La complejidad de la función en orden ascendente es, aproximadamente y comprobado de forma empírica, de O(n). 
En cuánto a las cotas, podemos observar de foma empírica como los valores de la subestimada tienden al infinito, los de la ajustada tienden a constante en un rango de valores entre 90 y 100 y los de la sobreestimada que tienden a 0.

***ins_sort*** desc
                                                     Subestimada    Ajustada  Sobreestimada
           n                  t(n) (ns)            t(n)/n^1.8       t(n)/n^2     t(n)/n^2.2
*        128                688640.0000            110.921134      42.031250      15.926865 (promedio de 10 repeticiones)
         256               2702200.0000            124.992960      41.232300      13.601586
         512              11133500.0000            147.892375      42.470932      12.196572
        1024              46511000.0000            177.425385      44.356346      11.089087
        2048             185718900.0000            203.452105      44.278836       9.636741
        4096             746301200.0000            234.782776      44.483018       8.427956
        8192            2976328800.0000            268.892698      44.350755       7.315147
       16384            11744272700.0000            304.698444      43.750825       6.282063

La complejidad de la función en orden descendente es, aproximadamente y comprobado de forma empírica, de O(n^2).
En cuánto a las cotas, podemos observar de foma empírica como los valores de la subestimada tienden al infinito, los de la ajustada tienden hacia una constante de 44 y los de la sobreestimada que tienden a 0.

***ins_sort*** alet
                                                     Subestimada    Ajustada  Sobreestimada
           n                  t(n) (ns)            t(n)/n^1.8       t(n)/n^2     t(n)/n^2.2
*        128                326190.0000             52.540318      19.909058       7.544122 (promedio de 10 repeticiones)
         256               1385600.0000             64.092312      21.142578       6.974450
         512               5770700.0000             76.655367      22.013474       6.321710
        1024              23791300.0000             90.756607      22.689152       5.672288
        2048              95537900.0000            104.660252      22.778010       4.957352
        4096             367325300.0000            115.558777      21.894294       4.148193
        8192            1504093900.0000            135.885480      22.412746       3.696724
       16384            5969912900.0000            154.885979      22.239659       3.193332

La complejidad de la función en orden aleatorio es, aproximadamente y comprobado de forma empírica, de O(n^2).
En cuánto a las cotas, podemos observar de foma empírica como los valores de la subestimada tienden al infinito, los de la ajustada tienden hacia una constante de 22 y los de la sobreestimada que tienden a 0.      

***shell_sort_hibbard*** asc
                                                     Subestimada    Ajustada  Sobreestimada
           n                  t(n) (ns)              t(n)/n^1     t(n)/n^1.2     t(n)/n^1.4
*        128                 81220.0000            634.531250     240.442382      91.110625 (promedio de 10 repeticiones)
*        256                191990.0000            749.960938     247.394847      81.609865 (promedio de 10 repeticiones)
*        512                476780.0000            931.210938     267.420118      76.796262 (promedio de 10 repeticiones)
        1024               1123500.0000           1097.167969     274.291992      68.572998
        2048               2628600.0000           1283.496094     279.337062      60.794259
        4096               5786200.0000           1412.646484     267.646460      50.709522
        8192              13197700.0000           1611.047363     265.723718      43.828068
       16384              28704500.0000           1751.983643     251.562591      36.121192

La complejidad de la función en orden ascendente es, aproximadamente y comprobado de forma empírica, de O(n^1.2).
En cuánto a las cotas, podemos observar de foma empírica como los valores de la subestimada tienden al infinito, los de la ajustada tienden a constante en un rango de valores entre 250 y 270 y los de la sobreestimada que tienden a 0.      

***shell_sort_hibbard*** desc
                                                     Subestimada    Ajustada  Sobreestimada
           n                  t(n) (ns)              t(n)/n^1     t(n)/n^1.2     t(n)/n^1.4
*        128                120590.0000            942.109375     356.992697     135.274936 (promedio de 10 repeticiones)
*        256                292750.0000           1143.554688     377.232364     124.440272 (promedio de 10 repeticiones)
*        512                720970.0000           1408.144531     404.383327     116.128616 (promedio de 10 repeticiones)
        1024               1726000.0000           1685.546875     421.386719     105.346680
        2048               3886400.0000           1897.656250     413.001429      89.884657
        4096               8822000.0000           2153.808594     408.070421      77.314887
        8192              19104900.0000           2332.141113     384.659831      63.445211
       16384              41646000.0000           2541.870117     364.980253      52.406527

La complejidad de la función en orden descendente es, aproximadamente y comprobado de forma empírica, de O(n^1.2).
En cuánto a las cotas, podemos observar de foma empírica como los valores de la subestimada tienden al infinito, los de la ajustada tienden a constante en un rango de valores entre 360 y 410, y los de la sobreestimada que tienden a 0.      

***shell_sort_hibbard*** alet
                                                     Subestimada    Ajustada  Sobreestimada
           n                  t(n) (ns)              t(n)/n^1    t(n)/n^1.25     t(n)/n^1.4
*        128                134070.0000           1047.421875     311.400387     150.396473 (promedio de 10 repeticiones)
*        256                329620.0000           1287.578125     321.894531     140.112733 (promedio de 10 repeticiones)
*        512                852780.0000           1665.585938     350.146311     137.359614 (promedio de 10 repeticiones)
        1024               2121700.0000           2071.972656     366.276479     129.498291
        2048               5116900.0000           2498.486328     371.402215     118.343660
        4096              11529400.0000           2814.794922     351.849365     101.042197
        8192              28492500.0000           3478.088379     365.589006      94.620369
       16384              61974700.0000           3782.635498     334.340901      77.987773

La complejidad de la función en orden aleatorio es, aproximadamente y comprobado de forma empírica, de O(n^1.25).
En cuánto a las cotas, podemos observar de foma empírica como los valores de la subestimada tienden al infinito, los de la ajustada tienden a constante en un rango de valores entre 310 y 370 y los de la sobreestimada que tienden a 0.      


Para intentar evitar datos anómalos, ejecutamos el código en un entorno lo más limpio posible, con el menor número de procesos en segundo plano, con un número de iteraciones totales de 10.
Al ejecutar el código no encontramos datos lo suficientemente diferenciados respecto al resto para considerarlos anómalos, ya que solo varían unas pocas unidades.

CONCLUSIONES

Después de comprobar que los algoritmos llevan al mismo resultado para cada  valor de entrada, analizamos la complejidad de cada algoritmo para saber cuál es el mas óptimo,es decir, el que utiliza menor tiempo de ejecución para la misma tarea.
Para cada algoritmo creamos varias cotas con el fin de averiguar su complejidad de manera empírica:
- Ordenación por inserción.
       - En orden ascendente. Hallamos las cotas n^0.8, n^1 y n^1.2.
       - En orden descendente. Hallamos las cotas n^1.8, n^2 y n^2.2.
       - En orden aleatorio. Hallamos las cotas n^1.8, n^2 y n^2.2.

- Ordenación Shell.
       - En orden ascendente. Hallamos las cotas n^1, n^1.2 y n^1.4.
       - En orden descendente. Hallamos las cotas n^1, n^1.2 y n^1.4.
       - En orden aleatorio. Hallamos las cotas n^1, n^1.25 y n^1.4.


Para establecer la eficiencia de cada algoritmo de forma empírica, comparamos las cotas y los tiempos de ejecución resultantes de ambos algoritmos en cada uno de los órdenes del vector.
Deducimos de los resultados obtenidos, que el algoritmo óptimo para resolver el problema para ordenar el vector en cada caso es:
- En orden ascendente. Resulta más eficiente el algoritmo ins_sort.
- En orden descendente. Resulta más eficiente el algoritmo shell_sort.
- En orden aleatorio. Resulta más eficiente el algoritmo shell_sort.

De los datos obtenidos, comprobamos que el algoritmo shell_sort es más eficiente en 2/3 casos, siendo el caso del vector ordenado el que supera la eficiencia del algoritmo shell_sort.
