Óscar Vilela Rodríguez y Ainhoa de Diego Silva 
oscar.vilela.rodriguez@udc.es; ainhoa.dediego.silva@udc.es

INTRODUCCIÓN

En esta cuarta práctica analizamos y demostramos de manera empírica la fiabilidad, tiempo de ejecución, y 
complejidad del algoritmo de Dijkstra propuesto, que realiza la busqueda del camino minimo (más corto, de modo que la suma de los pesos de las aristas sea la menor) entre los nodos de un grafo completo, no dirigído y ponderado
donde el peso de cada arista se inizializa de forma aleatoria, tomando valores de [1...1000] 

Esta función requiere como variable de entrada la matriz de adyacencia del grafo, que es la que nos proporciona todos los pesos de las aristas existentes.
Para ello fué necesario la creación de la función matrizAleatoria(), que es la que se usa para crear la matriz que representará el grafo,
es decir, la matriz de adyacencia donde se encontrarán los pesos [1...1000], excepto ls diagonal principal, ya que el grafo es simple (sin aristas múltiples, ni lazos)
Para ello, creamos un programa en Python, compuesto de varios archivos de código: 

Algoritmos.py 
FuncionesTests.py 
FuncionesAuxiliares.py 
Main.py

ACLARACIONES INICIALES

Para ejecutar el código debes moverte al directorio y escribir en la terminal python main.py.

Las características de la máquina que usamos se muestran a continuación: 

Nombre del modelo: Lenovo Legion 5 15ITH6H Intel Core i7-11800H/32GB/1TB SSD/RTX3070/15.6" 
Procesador Intel Core i7-11800H (8C / 16T, 2.3 / 4.6GHz, 24MB) 
Memoria RAM 2x 16GB SO-DIMM DDR4-3200 
Almacenamiento 1TB SSD M.2 2280 PCIe 3.0x4 NVMe 
Python 3.9.12

*Este programa en específico se ejecutó en el procesador (CPU) por lo que la tarjeta gráfica (GPU) no es necesaria para su ejecución. 
*Todas las medidas de tiempo han sido tomadas en nanosegundos (ns), mediante la función time.perf_counter_ns(), y 
 mostradas por pantalla a su vez en nanosegundos. 

*Todas las funciones aquí comentadas están descritas en mayor profundidad en el código. 


Una vez aclaradas las cuestiones básicas, explicaremos paso a paso, el proceso que nos llevó a las conclusiones finales de este informe.


DESARRROLLO DE LAS PRUEBAS



1º Creamos el archivo Algoritmos.py en el que definimos la función dijkstra.



2º Comprobamos el correcto funcionamiento del algoritmo y la complejidad algorítmica de forma empírica:
Para ello creamos el archivo FuncionesTests.py, que a su vez utiliza otras funciones definidas en FuncionesAuxiliares.py, donde definimos las funciones “test”: 

test

3º Para comprobar el tiempo de ejecución del algoritmo, creamos una función 
def test_tiempo_complejidad(alg, orden, exp1, exp2, exp3) que calcula
el tiempo y nos permite medir la complejidad de manera empírica gracias a las cotas.


test complejidad





Utilizamos un vector de progresión geométrica 2 porque nos ayuda a visualizar hacia qué valor de tiempo tiende el algoritmo.
Los asteriscos en las tablas de datos simbolizan que esos valores están por debajo del umbral de confianza, por lo que utilizamos 
el método del promedio para tener una medida más precisa.

Para intentar evitar datos anómalos, ejecutamos el código en un entorno lo más limpio posible, con el menor número de procesos en segundo plano, 
con un número de iteraciones totales de 10.

Para indicar los datos anómalos, los marcamos en las tablas de los tiempos de ejecución con el símbolo "#".

CONCLUSIONES
 
Para cada algoritmo creamos varias cotas con el fin de averiguar su complejidad de manera empírica:
- Creación de montículo.
       - En orden ascendente. Hallamos las cotas n^0.8, n^1 y n^1.2.
       - En orden descendente. Hallamos las cotas n^0.8, n^1 y n^1.2.
       - En orden aleatorio. Hallamos las cotas n^0.8, n^1 y n^1.2.

- Ordenación de un vector.
       - En orden ascendente. Hallamos las cotas n^1, n*log(n) y n^1.3.
       - En orden descendente. Hallamos las cotas n^1, n*log(n) y n^1.3.
       - En orden aleatorio. Hallamos las cotas n^1, n*log(n) y n^1.3.


Para establecer la eficiencia de cada algoritmo de forma empírica, comparamos las cotas y los tiempos de ejecución resultantes de ambos algoritmos 
en cada uno de los órdenes del vector.

De los datos obtenidos, comprobamos que el algoritmo de creación de montículo es más eficiente que el de ordenación de un vector, ya que su complejidad es lineal mientras que la de otra es O(n) = n*log(n).
Puede comprobarse más facilmente el crecimiento de las dos funciones en el siguiente enlace: https://m.wolframalpha.com/input?i=y+%3D+xlogx%2C++y%3Dx%5E1+%2C+x+from+1+to+100&lang=es
Así pues, vemos que la función n*log(n) tiene un crecimiento superior que la función x^1, comprobando así que el tiempo del algoritmo que crea los montículos es sustancialmente inferior al tiempo que tarda la otra función en ordenar vectores.
Aunque en este caso puede llegar a tener menos sentido la comparación entre los dos algoritmos, más hallá de resaltar su complejidad empírica, ya que realizan funciones diferentes.